---
layout: single
title: "SVD (Singular Vector Decomposition) with AI"
description: SVD(Singular Vector Decomposition)
date: "2022-04-18 11:30:00 +0900"
last_modified_at: "2022-04-18 21:30:00 +0900"
tags: ["Linear Algebra", "Matrix", "AI"]
---

---

# Decomposition of Matrix

Here **[Eigendecomsition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)** is a great source explaining essential definitions to know SVD better.
SVD is one of the model compression techniques that mostly used in AI field along with quantization, pruning, knowledge distillation, etc.
If you're interested in that, I recommend reading some survey or review papers such as 

- [Deng, Lei, et al. "Model compression and hardware acceleration for neural networks: A comprehensive survey."](https://ieeexplore.ieee.org/abstract/document/9043731)
- ["Choudhary, Tejalal, et al. "A comprehensive survey on model compression and acceleration."](https://link.springer.com/article/10.1007/s10462-020-09816-7)

> SVD (Singular Vector Decomposition)
---

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;A^{m\times&space;n}&space;=&space;U&space;\Sigma&space;V^{T}" title="https://latex.codecogs.com/svg.image?\LARGE A^{m\times n} = U \Sigma V^{T}" />

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;U^{m\times&space;m}" title="https://latex.codecogs.com/svg.image?\LARGE U^{m\times m}" />
= Left Singular Vector (Eigendecomposition of
<img src="https://latex.codecogs.com/svg.image?\large&space;AA^{T}" title="https://latex.codecogs.com/svg.image?\large AA^{T}" />
)  
<img src="https://latex.codecogs.com/svg.image?\LARGE&space;V^{n\times&space;n}" title="https://latex.codecogs.com/svg.image?\LARGE V^{n\times n}" />
 = Right Singular Vector (Eigendecomposition of 
<img src="https://latex.codecogs.com/svg.image?\large&space;A^{T}A" title="https://latex.codecogs.com/svg.image?\large A^{T}A" />
)  
<img src="https://latex.codecogs.com/svg.image?\LARGE&space;\Sigma" title="https://latex.codecogs.com/svg.image?\LARGE \Sigma" />
= Singular Values (Square root of Eigenvalues of
<img src="https://latex.codecogs.com/svg.image?\large&space;AA^{T}" title="https://latex.codecogs.com/svg.image?\large AA^{T}" />
or
<img src="https://latex.codecogs.com/svg.image?\large&space;A^{T}A" title="https://latex.codecogs.com/svg.image?\large A^{T}A" />
)

It can be a bit counterintuitive, so let's get right into the proof.


> Proof SVD
---

We have a matrix 
<img src="https://latex.codecogs.com/svg.image?\LARGE&space; A^{m\times&space;n}&space;" title="https://latex.codecogs.com/svg.image?\LARGE A^{m\times n}" />

First, 
<img src="https://latex.codecogs.com/svg.image?\LARGE&space;A^{T}A" title="https://latex.codecogs.com/svg.image?\LARGE A^{T}A" />
is symmetric and positive semidefinite.

---
The symmetricity : 
<img src="https://latex.codecogs.com/svg.image?\LARGE&space;(A^T&space;A)^T=A^T&space;A" title="https://latex.codecogs.com/svg.image?\LARGE (A^T A)^T=A^T A" />

The positive semidefinite : 
<img src="https://latex.codecogs.com/svg.image?\LARGE&space;x^T(A^T&space;A)x=(A^Tx)^T(A^Tx)\geq&space;0" title="https://latex.codecogs.com/svg.image?\LARGE x^T(A^T A)x=(A^Tx)^T(A^Tx)\geq 0" />

---

Therefore,
<img src="https://latex.codecogs.com/svg.image?\LARGE&space;(A^T&space;A)=V\Sigma&space;V^T" title="https://latex.codecogs.com/svg.image?\LARGE (A^T A)=V\Sigma V^T" />
by the spectral theorem.

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;(A^TA)v_i=\sigma&space;_i^2v_i&space;\;\;\;\;\;\;\;(i\leq&space;rank(A)\leq&space;n)" title="https://latex.codecogs.com/svg.image?\LARGE (A^TA)v_i=\sigma _i^2v_i \;\;\;\;\;\;\;(i\leq rank(A)\leq n)" />

Here, orthonormal vector v is an eigenvector corresponding the sigma value.

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;v_i^T(A^T&space;A)v_i=v_i^T\sigma&space;_i^2&space;v_i" title="https://latex.codecogs.com/svg.image?\LARGE v_i^T(A^T A)v_i=v_i^T\sigma _i^2 v_i" />

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;(Av_i)^T(Av_i)=\sigma&space;_i^2" title="https://latex.codecogs.com/svg.image?\LARGE (Av_i)^T(Av_i)=\sigma _i^2" />

---

Now we define a new vector
<img src="https://latex.codecogs.com/svg.image?\LARGE&space;u_i=\frac{Av_i}{\sigma&space;_i}&space;" title="https://latex.codecogs.com/svg.image?\LARGE u_i=\frac{Av_i}{\sigma _i} " />
,that is orthonormal vector due to

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;u_i^Tu_j=\frac{v_i^TA^TAv_j}{\sigma&space;_i\sigma&space;_j}=\left\{\begin{matrix}1,&space;\;&space;\;&space;\;&space;i=j\\&space;0,&space;\;&space;\;&space;\;&space;i\neq&space;j\end{matrix}\right." title="https://latex.codecogs.com/svg.image?\LARGE u_i^Tu_j=\frac{v_i^TA^TAv_j}{\sigma _i\sigma _j}=\left\{\begin{matrix}1, \; \; \; i=j\\ 0, \; \; \; i\neq j\end{matrix}\right." />

Recall 
<img src="https://latex.codecogs.com/svg.image?\LARGE&space;(A^TA)v_i=\sigma&space;_i^2v_i&space;(i\leq&space;rank(A)\leq&space;n)" title="https://latex.codecogs.com/svg.image?\LARGE (A^TA)v_i=\sigma _i^2v_i (i\leq rank(A)\leq n)" />
and multiply the matrix A to it.

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;AA^TAv_i=\sigma&space;_i^2Av_i" title="https://latex.codecogs.com/svg.image?\LARGE AA^TAv_i=\sigma _i^2Av_i" />

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;AA^Tu_i=\sigma&space;_i^2u_i" title="https://latex.codecogs.com/svg.image?\LARGE AA^Tu_i=\sigma _i^2u_i" />

Here, orthonormal vector u is an eigenvector corresponding the sigma value.

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;u_i^TAA^Tu_i=u_i^T\sigma&space;_i^2u_i" title="https://latex.codecogs.com/svg.image?\LARGE u_i^TAA^Tu_i=u_i^T\sigma _i^2u_i" />

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;(u_i^TA)(u_i^TA)^T=\sigma&space;_i^2" title="https://latex.codecogs.com/svg.image?\LARGE (u_i^TA)(u_i^TA)^T=\sigma _i^2" />

tells us that the magnitued(sigma) of u and v are the same.

---

Last part, from the definition of the vector u,  

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;Av_i&space;=&space;u_i\sigma&space;_i" title="https://latex.codecogs.com/svg.image?\LARGE Av_i = u_i\sigma _i" />

By expanding it to their corresponding dimension, we have

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;AV&space;=&space;U\Sigma" title="https://latex.codecogs.com/svg.image?\LARGE AV = U\Sigma" />

Sigma is diagonal matrix, which is filled with r (ranks) eigenvalues on the diagonal, otherwise 0s.

Since V is a orthonormal matrix, we can rewrite

<img src="https://latex.codecogs.com/svg.image?\LARGE&space;A=U\Sigma&space;V^T" title="https://latex.codecogs.com/svg.image?\LARGE A=U\Sigma V^T" />



> Python Implementation
---

Let's see it actually works anyway, here's a 5x5 matrix.
For simplicity, round it to the fourth digit after the decimal point.
```python
import numpy as np
from numpy.linalg import svd

a = np.random.rand(5, 5)
np.round(a, 4)
U, Sigma, V = np.linalg.svd(a)
```

<center>[Implementation Result]</center>
<p align="center">
  <img src="../assets/images/svd/SVD_array.png" alt="svd array" width="500"/>
</p>

And then, we can get its SVD matrices using numpy function below.
```python
U, Sigma, V = svd(a)
```

See the values with the same rounding and you can calculate them yourself if you don't trust the code.
```python
print('U matrix : ', np.round(U, 4))
print('Sigma : ', np.round(Sigma, 4))
print('V transpose matrix : ', np.round(V, 4))
```
<center>[Implementation Result]</center>
<p align="center">
  <img src="../assets/images/svd/decomposition.png" alt="decomposition components" width="450"/>
</p>

`Sigma` here is a vector unit and values inside (singular values) are mostly sorted in descending order.
Since we have 5 values, it's "5-Rank".
For matrix multiplcation, convert `Sigma` into matrix form to reconstruct the original matrix `a`.
```python
Sigma_mat = np.diag(Sigma)
a_reconstructed = np.matmul(np.matmul(U, Sigma), V)
print(a_reconstructed)
```
<center>[Implementation Result]</center>
<p align="center">
  <img src="../assets/images/svd/reconstructed_SVD_array.png" alt="reconstructed array" width="450"/>
</p>


> Application on Image
---

As we use the whole (all ranks) singular values, we can fully reconstruct the original matrix, that's called full SVD.
Depending on the number of ranks to be used, it's also called thin, compact and truncated SVD.
Because of the descending order, the last few values are likely to have little significance on the reconstruction.
In other words, we can selectively use the multiple largest values to reconstruct with small information loss..
Here's an example, we have 400x400 image on a greyscale of 0 ~ 255 (8-bit).

<center>[Original Image]</center>
<p align="center">
  <img src="../assets/images/svd/monkey_original.png" alt="monkey original" width="450"/>
</p>

Below are "100-rank" and "50-rank" reconstrction images of the original, we can see the most of the information is reserved. 
<center>[100 Rank]</center>
<p align="center">
  <img src="../assets/images/svd/monkey_100.png" alt="monkey 100 rank" width="450"/>
</p>

<center>[50 Rank]</center>
<p align="center">
  <img src="../assets/images/svd/monkey_50.png" alt="monkey 50 rank" width="450"/>
</p>
---
Still, we can somehow tell what the object in the photo is.
Now it's getting hard to tell whether it's monkey or human.
And it's obvious that there's some significant loss of information.

<center>[30 Rank]</center>
<p align="center">
  <img src="../assets/images/svd/monkey_30.png" alt="monkey 30 rank" width="450"/>
</p>

In "10 Rank", we barely see the contour of the object, or might not know if there's an object.

<center>[10 Rank]</center>
<p align="center">
  <img src="../assets/images/svd/monkey_10.png" alt="monkey 10 rank" width="450"/>
</p>

---
Then, how much has the data been compressed?
We can manually choose the number of rank using this trade-off.
Inside the sum-of-products of SVD indicates U, sigma and V respectively.

| # of Rank |    Required data (# of 8-bit)   | Compression rate |
|:---------:|:-------------------------------:|:----------------:|
|  Original |         400x400 = 160000        |         -        |
|  100 Rank | 400x100 + 100 + 100x400 = 80100 |      50.01%      |
|  50 Rank  |   400x50 + 50 + 50x400 = 40050  |      25.03%      |
|  30 Rank  |   400x30 + 30 + 30x400 = 24030  |      15.02%      |
|  10 Rank  |   400x10 + 10 + 10x400 = 8010   |       5.01%      |

Aside from this superficial relationship, if we consider the whole computation process of SVD, it might not be a good choice for the actual hardware-implementation.
Specifically, the SVD procedure requires
- <u>Solving polynomial equations</u> for eigenvalues and eigenvectors
- <u>Sorting (or choosing the first 'rank' values)</u> the eigenvalues in descending order
- <u>Rearrange matrices</u> based on the previous information

Therefore, most of AI Accelerator research papers dealing with SVD assume that the procedure is already performed at compile-time or CPU.
When it comes to reconstruction, it is just matrix multiplcation.
So, the reconstruction can be feasibly performed in general MAC (Multiply-and-Accumulate) unit.
